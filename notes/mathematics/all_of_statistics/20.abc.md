*Bias and variance*
Concept: dataset that split to k-fold, k $\to$ $\infty$
* Bias: the estimators that learn (compute )from each fold, on average will be close to true value of the parameter(function).
            $$b(x) = \mathbb{E}(\hat{g}_n(x)) - g(x)$$
  *   $b(x)$ large : hight bias $\to$ model can't learn from data
* Variance: the estimators are stable from different training folds.
    $$v(x) = \mathbb{V}(\hat{g}_n(x)) = \mathbb{E}((\hat{g}_n(x) - \mathbb{E}(\hat{g}_n(x)))^2) $$
    
    * $v(x)$: large : model overfit on each fold of dataset. 
> **NOTE**  $$RISK = BIAS^2 + VARIANCE $$

*Markov chain* 